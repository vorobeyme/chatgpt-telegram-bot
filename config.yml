debug: ${APP_DEBUG}

telegram:
  token: ${TELEGRAM_TOKEN}
  replyToMessage: false

# ChatGPT engine configurations
chatGPT:
  apiKey: ${OPENAI_API_KEY}
  # ID of the model to use.
  # You can use the List models API (https://platform.openai.com/docs/api-reference/models/list) to see all of your available models,
  # or see Model overview (https://platform.openai.com/docs/models/overview) for descriptions of them.
  model: 'text-davinci-003'
  # The maximum number of tokens to generate in the completion.
  # The token count of prompt plus max_tokens cannot exceed the model's context length.
  # Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
  # Defaults to 16
  maxTokens: 100
  # What sampling temperature to use.
  # Higher values means the model will take more risks.
  # Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
  # We generally recommend altering this or top_p but not both.
  # Defaults to 1
  temperature: 0
  # An alternative to sampling with temperature, called nucleus sampling,
  # where the model considers the results of the tokens with top_p probability mass.
  # So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  # We generally recommend altering this or temperature but not both.
  # Defaults to 1
  topP: 1
  # How many completions to generate for each prompt.
  # Note: Because this parameter generates many completions, it can quickly consume your token quota.
  # Use carefully and ensure that you have reasonable settings for max_tokens and stop.
  # Defaults to 1
  n: 1
  # Up to 4 sequences where the API will stop generating further tokens.
  # The returned text will not contain the stop sequence.
  #Defaults to null
  stop: null
  # Number between -2.0 and 2.0.
  # Positive values penalize new tokens based on whether they appear in the text so far,
  # increasing the model's likelihood to talk about new topics.
  # Defaults to 0
  presencePenalty: 0
  # Number between -2.0 and 2.0.
  # Positive values penalize new tokens based on their existing frequency in the text so far,
  # decreasing the model's likelihood to repeat the same line verbatim.
  # Defaults to 0
  frequencyPenalty: 0
